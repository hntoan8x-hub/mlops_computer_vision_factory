# ðŸ“˜ CV Factory â€“ End-to-End Operational Blueprint

## ðŸŽ¯ Objective
This document details the **full operational flow** of a Computer Vision production system (e.g., SADS â€“ Surface Anomaly Detection System). It follows the data lifecycle from **video capture â†’ AI model inference â†’ decision execution â†’ feedback learning**.

---

## âš™ï¸ I. Data Capture & Ingestion
### ðŸŽ¥ Step 1: Video Stream Input
- Industrial camera streams 60 FPS video.
- Each frame contains one or more products.

### âš™ï¸ Step 2: VideoProcessingOrchestrator
Responsible for converting video to a list of processed image frames.

```
VideoProcessingOrchestrator
 â”œâ”€â”€ BaseVideoSampler          â†’ Select key frames
 â”œâ”€â”€ BaseVideoCleaner          â†’ Remove noise, stabilize lighting
 â”œâ”€â”€ VideoFrameResizer         â†’ Standardize frame dimensions
 â””â”€â”€ Output: List[Frames]
```
**Result:** 4D video tensor â†’ List of 3D image tensors.

---

## ðŸ§© II. Image Preprocessing & Augmentation
### ðŸ§¼ Step 3: ImageProcessingOrchestrator
Cleans, normalizes, and augments frames before modeling.

```
ImageProcessingOrchestrator
 â”œâ”€â”€ CleanerFactory â†’ ResizeCleaner, NormalizationCleaner
 â”œâ”€â”€ AugmenterFactory â†’ FlipRotate, CutMix, MixUp, NoiseInjection
 â”œâ”€â”€ FeatureExtractorFactory â†’ HOG, ORB, CNNFeatureExtractor
 â””â”€â”€ Output: Normalized Tensor (3x224x224)
```

**Outcome:** Ready-to-train / ready-to-infer tensors.

---

## ðŸ§  III. Feature Extraction & Embedding
Transforms visual data into structured numerical representations.

| Component | Purpose | Example |
|------------|----------|----------|
| FeatureExtractor | Extract spatial features (local patterns) | CNN / HOG / ORB |
| Embedder | Generate semantic embeddings | ViT / CLIP / AutoEncoder |

**Result:**
```
Image Tensor (3x224x224) â†’ Embedding Vector (1x1024)
```

---

## ðŸ§® IV. Training Pipeline
### ðŸ§± Step 4: Trainer Orchestrator
Handles model learning and lifecycle management.

```
Trainer Orchestrator (cnn_trainer.py / depth_trainer.py)
 â”œâ”€â”€ DataLoader â†’ Train / Val / Test sets
 â”œâ”€â”€ Optimizer â†’ AdamW, SGD
 â”œâ”€â”€ Scheduler â†’ LR Warmup, CosineDecay
 â”œâ”€â”€ Loss Functions â†’ CrossEntropy, L1, SSIM
 â”œâ”€â”€ DDP Handling â†’ BaseDistributedTrainer
 â”œâ”€â”€ Checkpoint & Logging â†’ MLflow Tracker
 â””â”€â”€ Output: Trained Model (artifact.pt)
```

**Outcome:** Saved model artifact, e.g., `sads_resnet50_v1.pt`

---

## ðŸ“Š V. Evaluation & Output Adapter
### ðŸ§© Step 5: EvaluationOrchestrator
Calculates model performance metrics.

| Task | Key Metrics |
|------|--------------|
| Classification | Accuracy, Precision, Recall |
| Detection | mAP, IoU |
| Depth Estimation | RMSE, AbsRel, Î´â‚, Î´â‚‚ |

### ðŸ”„ Step 6: OutputAdapter
Standardizes raw model outputs for downstream use.

```
Raw Model Output (logits/tensors)
   â†“
OutputAdapter
   â†“
[
  {"bbox": [x_min, y_min, x_max, y_max], "score": 0.92, "class": "scratch"},
  {"bbox": [x_min, y_min, x_max, y_max], "score": 0.88, "class": "hole"}
]
```

---

## â˜ï¸ VI. Model Registry & Deployment
### ðŸš€ Step 7: Model Registration (MLflow)
- Registers model + metadata + version.
- Example entry:
  ```
  Model: SADS_ResNet50
  Version: v2.1
  Metrics: mAP=0.86, IoU=0.78
  ```

### ðŸ§­ Step 8: Deployment Orchestrator
Automates serving environment rollout.

```
DeploymentOrchestrator
 â”œâ”€â”€ deploy_standard.py â†’ Standard deployment (full rollout)
 â”œâ”€â”€ run_canary_rollout.py â†’ Gradual traffic shift (5% â†’ 100%)
 â”œâ”€â”€ rollback_deployment.py â†’ Rollback to stable version
 â””â”€â”€ Output: Serving Endpoint (API)
```

**Result:** API Endpoint: `https://cv-factory/api/v1/sads/inference`

---

## âš¡ VII. Decision Engine & Feedback Loop
### ðŸ¤– Step 9: Decision Engine
Integrates AI inference into physical control and quality loops.

```
DecisionEngine
 â”œâ”€â”€ Read OutputAdapter results
 â”œâ”€â”€ Apply confidence thresholds
 â”œâ”€â”€ If defect detected â†’ send signal to PLC (Reject Product)
 â”œâ”€â”€ Else â†’ QC Logger marks product as PASS
 â””â”€â”€ All results logged to database
```

### ðŸ” Step 10: Feedback & Continuous Learning
Integrates drift detection and auto-retraining.

```
monitor_and_trigger.py
 â”œâ”€â”€ Detect performance drift via Prometheus metrics
 â”œâ”€â”€ If drift > threshold â†’ trigger Airflow retraining DAG
 â””â”€â”€ New model registered + deployed automatically
```

---

## ðŸ“˜ VIII. End-to-End Data Flow Diagram

```
Video Stream
   â†“
VideoProcessingOrchestrator
   â†“
ImageProcessingOrchestrator
   â†“
FeatureExtractor / Embedder
   â†“
Trainer (Learning Loop)
   â†“
EvaluationOrchestrator + OutputAdapter
   â†“
Model Registry + Deployment
   â†“
Decision Engine (AI â†’ Physical Action)
   â†“
Feedback Collector â†’ Retraining
```

---

## ðŸ§± IX. Final System Deliverables
| Output Artifact | Description | Generated By |
|-----------------|--------------|---------------|
| `sads_model.pt` | Trained CNN model | Trainer |
| `evaluation_report.json` | Performance metrics | EvaluationOrchestrator |
| `mlflow registry entry` | Model + metadata | MLflowLogger |
| `serving endpoint` | Inference REST API | DeploymentOrchestrator |
| `plc_signal` | Command for mechanical reject system | DecisionEngine |
| `feedback dataset` | Data for retraining | Monitoring Layer |

---

## ðŸ§­ X. System Summary (Text Diagram Overview)

```
Camera (Video Input)
   â†“
VideoProcessingOrchestrator â†’ Frame Sampler / Cleaner / Resizer
   â†“
ImageProcessingOrchestrator â†’ Augmenter / Normalizer / FeatureExtractor
   â†“
Trainer â†’ Model Training / Evaluation / Registry
   â†“
DeploymentOrchestrator â†’ Serve Model via API Endpoint
   â†“
DecisionEngine â†’ PLC Signal / QC Dashboard / Feedback Loop
   â†“
Monitoring â†’ Auto Retraining â†’ New Version Deployment
```

---

âœ… **Conclusion:**  
Your CV Factory system is no longer just a training pipeline â€” itâ€™s a **fully hardened AI Production Ecosystem**.  
It can process real-time video, infer surface defects, take autonomous actions, and continuously improve over time.

